---
title: "CPTR330 -- Final Project"
author: "Ryan Rabello"
date: "June 6, 2017"
output:
  pdf_document:
    number_section: yes
course: CPTR330
---

```{r, include=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

# Titanic Data
## Step 1 - Collecting the data

This dataset has been made available to us by kaggle. Full details to the dataset can be found [here](https://www.kaggle.com/c/titanic/).

## Importing data.
First things first we need to do is import our data to take a look at it. 
```{r}
# This function gets the target csv file from the url. If that fails it uses a copy of the files stored locally. 
get <- function(fileName) {
  
  return(read.csv(paste0("https://cs.wallawalla.edu/~carmpr/cptr330/titanic/", fileName), stringsAsFactors = TRUE))
  
  # If the file is sucessfully found on the server download it, otherwise use a local copy of the files.
  #file <- tryCatch({
  # return(read.csv(paste0("https://cs.wallawalla.edu/~carmpr/cptr330/titanic/", fileName), stringsAsFactors = TRUE))
  #}, error = function(e) {
  # return(read.csv(paste0("./data/titanic/", fileName), stringsAsFactors = TRUE))
  #})
  #return(file)
}
# The train.csv contains all features and is used to train models. 
raw_train <- get("train.csv")

# The test.csv is not going to change during the final time. 
raw_test <- get("test.csv")

# The test_final.csv is what is going to be used (like a kaggle submission) to grade the performance of our algorithm. 
raw_test_final <- get("test_final.csv")

# The test_results.csv contains the labels (or answers) to test.csv.
raw_test_results <- get("test_results.csv")

# The test_results_final.csv contains the labels for test_final.csv.
raw_test_results_final <- get("test_results_final.csv")

# Take a look at the data.
str(raw_train)
```

## Step 2 - Exploring And Preparing The Data

The following list briefly explains what each variable is and what type (categorical or regression) it is.

* **Survived**	A Boolean (0 or 1) indicating the survival of this particular passenger. (Categorical)
* **pclass**	Ticket class	1 = 1st, 2 = 2nd, 3 = 3rd (A numerical representation of the) (Categorical)
* **sex**	Gender of the individual. (Categorical)
* **Age**	Age in floating point years. (Regression)
* **sibsp**	# of siblings / spouses aboard the Titanic (regression)
* **parch**	# of parents / children aboard the Titanic (regression) 
* **ticket**	Ticket number (ID)
* **fare**	Passenger fare (the cost of the ticket). (Regression)
* **cabin**	Cabin number (String ex "A15" and "B12") (Categorical)
* **embarked**	Port of Embarkation	C = Cherbourg, Q = Queenstown, S = Southampton (Categorical)

Because `PassengerId`, `Name` and `ticket` are mostly unique we they will not be useful in any machine learning calculation and are therefore nullified. I'm also changing some of the names of variables so that everything works okay. Lastly I'm separating the `Cabin` variable into two variables `Floor` and `Room`.
```{r}
process <- function (titanic) {
  # Nullify unique feilds (they are still stored for later use.)
  titanic$PassengerId <- NULL
  titanic$Name <- NULL
  titanic$Ticket <- NULL
  
  # Convert some variables to factors 
  levels(titanic$Embarked) <- c("Missing", "Cherbourg", "Queenstown",  "Southampton")
  titanic$Pclass <- as.factor(titanic$Pclass)
  if ("Survived" %in% names(titanic)){
    titanic$Survived <- factor(titanic$Survived, c(0, 1), c("Died", "Survived"))
  }
  # Remove the suprise variable.
  if ("X" %in% names(titanic)){
    titanic$X <- NULL
  }
  
  # Convert Cabin to `Room` (this line will introduce NAs)
  titanic$Room <- as.numeric(substring(as.character(titanic$Cabin), 2))
  
  # Convert cabin to "floor" (the first letter of each cabin)
  cabinLev <- levels(titanic$Cabin)
  cabinLev <- substr(cabinLev, 1,1)
  levels(titanic$Cabin) <- cabinLev
  levels(titanic$Cabin) <- c("?","A", "B", "C", "D", "E", "F", "G", "T")
  titanic$Floor <- titanic$Cabin
  
  # Remove Cabin becuase it is represented above. 
  titanic$Cabin <- NULL
  
  return(titanic)
}

# Process the new data. 
train <- process(raw_train)
test <- process(raw_test)
test_final <- process(raw_test_final)

# 
test_results <- factor(raw_test_results$Survived, c(0, 1), c("Died", "Survived"))
test_results_final <- factor(raw_test_results_final$Survived, c(0, 1), c("Died", "Survived"))

test_all <- cbind(Survived = test_results_final, test_final)

titanic <- rbind(train, test_all)

```
## Global Functions
A global function for exporting data. `data` is a vector of answers and `name` the name of the csv that will be exported. (ex. name = "tree" -> "tree.csv")
```{r}
export <- function(data,name) {
  kaggle <- data.frame(PassengerId = raw_test_final$PassengerId, Survived = as.numeric(data) - 1)
  write.csv(x = kaggle, file = paste("./exports/",  name, ".csv", sep = ""), row.names = F, quote = F)
}

# Dynamically install/import a package
dynInstall <- function(package) {
  if(package %in% rownames(installed.packages())) {
    do.call('library', list(package))
  } else {
    install.packages(package)
    do.call("library", list(package))
  }
}
```

## Exploring the data

```{r}
dynInstall('ggplot2')

dynInstall('ggthemes')

# Styles for the graphs. 
# ggplot <- function(...) ggplot2::ggplot(...) + scale_color_fivethirtyeight("cyl") + theme_fivethirtyeight()
# ggplot <- function(...) ggplot2::ggplot(...) + theme_hc() + scale_colour_hc()

# Graph a histogram of Survival
ggplot(titanic, aes(x = Survived)) + geom_bar() + ggtitle("Survival")

# Graph some continuous variables in a "Violin" graph.
ggplot(titanic, aes(Survived, Fare)) + geom_violin(aes(fill = Survived), scale = "area") + ggtitle("Fare related to survival")
ggplot(titanic, aes(Survived, Age)) + geom_violin(aes(fill = Survived), scale = "area") + ggtitle("Age related to survival")
ggplot(titanic, aes(Fare, fill = Survived)) + geom_histogram() + ggtitle("Fare Frequency vs Survival")
ggplot(titanic, aes(Age, fill = Survived)) + geom_histogram() + ggtitle("Age Frequency vs Survival")
ggplot(titanic, aes(Age, fill = Survived)) + geom_histogram(binwidth = 1) + ggtitle("Age Frequency vs Survival")
ggplot(titanic, aes(Age, ..density.., colour = Survived)) + geom_freqpoly(binwidth = 2)

# Plot a jitter graph for the discrete variables.
ggplot(titanic, aes(Embarked, Survived)) + geom_jitter() + ggtitle("Embarked vs Survival")
ggplot(titanic, aes(Pclass, Survived)) + geom_jitter() + ggtitle("Person Class vs Survival")
ggplot(titanic, aes(Sex, Survived)) + geom_jitter() + ggtitle("Gender vs Survival")


# These are "3d" graphs that show two variables and the classificiation (survived or didn't survive) is shown as the color of the point.
ggplot(titanic, aes(x = Floor, y = Room, colour = Survived)) + geom_jitter() + ggtitle("Cabin vs Survival")
ggplot(titanic, aes(x=SibSp, y=Parch, colour = Survived)) + geom_jitter() + ggtitle("Family vs Survival") +labs(x="Siblings/Spouse", y="Parents/Children")

# Added after running algorithms
ggplot(titanic, aes(x = Pclass, y = Sex, colour = Survived)) + geom_jitter() + ggtitle("Class and Gender vs Survival")
ggplot(titanic, aes(x = Embarked, y = Pclass, colour = Survived)) + geom_jitter() + ggtitle("Class and Gender vs Survival")
```

All the graphs (except the gender graph) above seem to indicate that the titanic data doesn't have a very recognizable pattern. In other words there is a lot of noise. The algorithms that will then work better are ones like Decision Trees, Neural Networks, and random forest. Also because so many variables in this dataset are discrete a feature ML algorithm like Naive Bayes should also be considered. 

```{r}
# Correlation 
dynInstall('corrplot')

# Replace NA with the mean of that row.
matrix <- lapply(titanic, function(x) {
  x <- as.numeric(x)
  avg <- mean(x, na.rm = T)
  x[is.na(x)] <- avg
  return(x)
})

matrix <- data.matrix(titanic, rownames.force = T)

# Somehow NAs were introduced to lets set those to zero. 
matrix[is.na(matrix)] <- 0

# Calculate the correlation matrix.
corr_titanic <- cor(matrix)

# Graph the correlation matrix.
corrplot(corr_titanic, method = "circle")
```

All of variables (excluding gender) are not linearly dependent on the graph. This means that a machine learning algorithm like a linear regression will not work very well for this dataset.

# Step 3 - Training A Model On The Data

Of the models I tested out the most successful one was the Decision Tree. The other models are included here for clarity. These other models include: Naive bays, which performed the worse out of the four that I tested; Neural Networks, performed the second best right below decision trees; and Random Forest, which I wasn't able to predict any values for.  

# Descision Trees

## Training
```{r}
dynInstall("C50")

# Train the model. 
tree <- C5.0(train[,-1], train$`Survived`, trials = 5)

summary(tree)
# plot(tree)
```


## Step 4 - Evaluating Model Performance
```{r}
tree_pred <- function(test, answer) {
  prediction <- predict(tree, test)
  xtab <- table(prediction, answer)
  return(xtab)
}

tree_test <- tree_pred(test, test_results)
tree_test
tree_test_final <- tree_pred(test_final, test_results_final)
tree_test_final
```

The initial testing of the Decision tree was very good however, I was able to increase it's accuracy a little bit by increasing the number of trials and a couple other parameters as seen below. 

## Step 5 - Improving Model Performance
```{r}
tree <- C5.0(train[,-1], train$`Survived`, trials = 10, control = C5.0Control(bands = 50), rules = T)

summary(tree)
# plot(tree)

tree_pred <- function(test, answer) {
  prediction <- predict(tree, test)
  xtab <- table(prediction, answer)
  return(xtab)
}


tree2_test <- tree_pred(test, test_results)
tree2_test
tree2_test_final <- tree_pred(test_final, test_results_final)
tree2_test_final


```

## Kaggle Exporting
```{r}
tree_pred <- predict(tree, test_final)
export(tree_pred, "tree")
```

# Final Scoring

```{r}
# Kaggle Score
tree2_test
accuracy_kaggle <- sum(diag(tree2_test))/sum(tree2_test)
accuracy_kaggle
```

```{r}
# Final Score
tree2_test_final
accuracy_final <- sum(diag(tree2_test_final))/sum(tree2_test_final)
accuracy_final
```

# Other Algorithms

Much work was put into increasing the accuracy of these algorithms. However, they didn't perform better than decision trees so they are included here for clarity.

## Naive Bayes

```{r}
# Load e1071 if it's not installed install it. 
dynInstall('e1071')

# Trains the data set.
nb1 <- naiveBayes(Survived~., data=train)
#nb1
```

## Testing 
```{r}
nb_guess <- predict(nb1, test[,c("Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Floor", "Room")])
table(nb_guess, test_results_final)

# Calculate the percentage of correct guesses.
nb_correct <- nb_guess == test_results_final
table(nb_correct)/length(nb_correct) * 100

export(nb_guess, "nb")
```

## Neural Networks

Neural networks only work with numerical data so I'm going to for now, remove the categorical data. An option to do later would be to associate a category to a set of nodes and activate the corresponding input node for the specific category.
```{r}
nn_process <- function(titanic){
  
  for(key in c("Embarked", "Pclass")) {
    # Convert those factors into boolean data.frames
    a <- NULL
    for(level in levels(titanic[[key]])) {
      a[[level]] <- as.numeric(titanic[key] == level)
    }
    # Add cbind all those data.frames together
    titanic <- cbind(titanic, as.data.frame(a))
    titanic[key] <- NULL
  }
  
  # Covert Everything to a numeric.
  titanic <- lapply(titanic, function(x) as.numeric(x))
  
  # Define our Normalize function
  normalize <- function(x) {
    if (anyNA(x) || max(x) == min(x)) {
      x <- scale(x, center = FALSE, scale = TRUE)
      x[is.na(x)] <- 0
      return(x)
    } else {
      return ((x - min(x)) / (max(x) - min(x)))
    }
  }
  
  titanic <- lapply(titanic, normalize)
  titanic <- data.frame(titanic)
  return(titanic)
}

nn_train <- nn_process(train)
nn_test <- nn_process(test)
```

### Training the neural network
```{r}
# Dynamically load `neuralnet`
dynInstall('neuralnet')

set.seed(1234)

# Create a formula in the form "Survived ~ V(1) + V(2) + ... + V(n-1) + V(n)"
fmla <- as.formula(paste("Survived ~ ", paste(names(nn_test), collapse= "+")))

# Train the model
nn <- neuralnet(fmla , data = nn_train, hidden = c(3), threshold = 0.05)
plot(nn)
```

```{r}
# Predict some data 
nn_predict <- compute(nn, subset(nn_train, select=-c(Survived)))
cor(nn_predict$net.result, as.numeric(nn_train$Survived) - 1)
```

## Process and export data
```{r}
nn_predict <- compute(nn, nn_test)
results <- round(nn_predict$net.result)
results[results < 0] <- 0
results[results > 1] <- 1
results <- factor(x = results, c(0,1), c("Dead", "Survived"))

export(results, "nn")
```

## Random Forest
```{r}

dynInstall("randomForest")

rf_process <- function(titanic) {
  remove_na <- function(x) {
    if(class(x) == "factor"){
      return(x)
    } else {
      x[is.na(x)] <- 0
      return(x)
    }
  }
  return(lapply(titanic, remove_na))
  
}

rf_train <- as.data.frame(rf_process(train))
rf_test <- as.data.frame(rf_process(test))

fmla <- as.formula(paste("Survived ~ ", paste(names(rf_train), collapse= "+")))

rand_forest <- randomForest(fmla, data = rf_train, na.action = na.omit)
print(rand_forest)
# rf_predict <- predict(rand_forest, rf_test$Age)
```
Unfortunately random forests isn't working. Otherwise I'm would love to see the results it has.

# Conclusion
```{r}
# Kaggle Score (based on `test`)
tree2_test
accuracy_kaggle
```

```{r}
# Final Score (based on `test_final`)
tree2_test_final
accuracy_final
```
